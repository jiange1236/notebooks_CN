{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ï¼ƒFalcon H1çš„ä¸èˆ’æœåŸ¹è®­\n",
        "\n",
        "è¯¥ç¬”è®°æœ¬æ˜¯ç”±TII Falconå›¢é˜Ÿæ’°å†™çš„ã€‚\n",
        "æœ‰å…³Falcon H1ç³»åˆ—æ¨¡å‹çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼š\n",
        "1ã€‚xx_markDown_link_xx\n",
        "2ã€‚xx_markDown_link_xx\n",
        "3ã€‚xx_markDown_link_xx\n",
        "4ã€‚xx_markDown_link_xx\n",
        "\n",
        "è¦è¿è¡Œæ­¤æ“ä½œï¼Œè¯·æŒ‰â€œ*Runtime*â€ï¼Œç„¶åæŒ‰â€œ*è¿è¡Œ*â€ ** free ** tesla Tesla T4 Google COLABå®ä¾‹ï¼\n",
        "<div class =â€œ Align-Centerâ€>\n",
        "<a href =â€œ https://unsloth.ai/â€> <img src =â€œ https://github.com/unslothai/unslothai/unsloth/raw/raw/main/main/mains/unsloth%20new%20new%20logo.png.pngâ€\n",
        "<a href =â€œ https://discord.gg/unslothâ€> <img src =â€œ https://github.com/unslothai/unslothai/unsloth/raw/main/main/main/images/images/discord button.png button.png\n",
        "<a href =â€œ https://docs.unsloth.ai/â€> <img src =â€œ https://github.com/unslothai/unslothai/unsloth/unsloth/main/main/main/mains/images/documentation%20green%20green%20breen%20button.png?png?raw=raw=true width widthâ€ <i>åœ¨<a href =â€œ https://github.com/unslothai/unslothâ€> github </a> </i>â­ä¸­\n",
        "</div>\n",
        "\n",
        "è¦åœ¨æ‚¨è‡ªå·±çš„è®¡ç®—æœºä¸Šå®‰è£…ä¸å¡ï¼Œè¯·æŒ‰ç…§æˆ‘ä»¬çš„githubé¡µé¢ä¸Šçš„å®‰è£…è¯´æ˜[Official Page](https://tiiuae.github.io/Falcon-H1/)ä¸Šçš„å®‰è£…è¯´æ˜ã€‚\n",
        "\n",
        "æ‚¨å°†å­¦ä¹ å¦‚ä½•åš[Blogpost](https://falcon-lm.github.io/blog/falcon-h1/)ï¼Œå¦‚ä½•[Official Github Page ](https://github.com/tiiuae/Falcon-H1)ï¼Œå¦‚ä½•[HF Collection](https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df)ï¼Œï¼†xx_markDown_link_xx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### æ¶ˆæ¯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "** New ** Unsplothç°åœ¨æ”¯æŒåŸ¹è®­OpenAiçš„æ–°** GPT-oss **æ¨¡å‹ï¼æ‚¨å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„** [Colab notebook](https://x.com/UnslothAI/status/1953896997867729075)å…è´¹å¯åŠ¨Finetune GPT-oss **ï¼\n",
        "\n",
        "Unsplothç°åœ¨æ”¯æŒæ–‡æœ¬å¯¹è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚é˜…è¯»æˆ‘ä»¬çš„xx_markDown_link_xxã€‚\n",
        "\n",
        "é˜…è¯»æˆ‘ä»¬çš„** [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning) **å¹¶æŸ¥çœ‹æˆ‘ä»¬çš„æ–°** [Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune) ** Quantsä¼˜å…ˆé‡ä¼˜äºå…¶ä»–é‡åŒ–æ–¹æ³•ï¼\n",
        "\n",
        "è¯·è®¿é—®æˆ‘ä»¬çš„æ‰€æœ‰æ–‡æ¡£ï¼Œä»¥è·å–æˆ‘ä»¬çš„æ‰€æœ‰XX_MarkDown_link_xxå’Œ[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### å®‰è£…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%",
        "%",
        "c",
        "a",
        "p",
        "t",
        "u",
        "r",
        "e",
        "\n",
        "i",
        "m",
        "p",
        "o",
        "r",
        "t",
        " ",
        "o",
        "s",
        "\n",
        "o",
        "s",
        ".",
        "e",
        "n",
        "v",
        "i",
        "r",
        "o",
        "n",
        "[",
        "\"",
        "C",
        "U",
        "D",
        "A",
        "_",
        "V",
        "I",
        "S",
        "I",
        "B",
        "L",
        "E",
        "_",
        "D",
        "E",
        "V",
        "I",
        "C",
        "E",
        "S",
        "\"",
        "]",
        " ",
        "=",
        " ",
        "\"",
        "0",
        "\"",
        "\n",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "p",
        "i",
        "p",
        "3",
        "-",
        "a",
        "u",
        "t",
        "o",
        "r",
        "e",
        "m",
        "o",
        "v",
        "e",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "t",
        "o",
        "r",
        "c",
        "h",
        " ",
        "t",
        "o",
        "r",
        "c",
        "h",
        "v",
        "i",
        "s",
        "i",
        "o",
        "n",
        " ",
        "t",
        "o",
        "r",
        "c",
        "h",
        "a",
        "u",
        "d",
        "i",
        "o",
        " ",
        "x",
        "f",
        "o",
        "r",
        "m",
        "e",
        "r",
        "s",
        " ",
        "-",
        "-",
        "i",
        "n",
        "d",
        "e",
        "x",
        "-",
        "u",
        "r",
        "l",
        " ",
        "h",
        "t",
        "t",
        "p",
        "s",
        ":",
        "/",
        "/",
        "d",
        "o",
        "w",
        "n",
        "l",
        "o",
        "a",
        "d",
        ".",
        "p",
        "y",
        "t",
        "o",
        "r",
        "c",
        "h",
        ".",
        "o",
        "r",
        "g",
        "/",
        "w",
        "h",
        "l",
        "/",
        "c",
        "u",
        "1",
        "2",
        "4",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "u",
        "n",
        "s",
        "l",
        "o",
        "t",
        "h",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "-",
        "-",
        "u",
        "p",
        "g",
        "r",
        "a",
        "d",
        "e",
        " ",
        "t",
        "r",
        "a",
        "n",
        "s",
        "f",
        "o",
        "r",
        "m",
        "e",
        "r",
        "s",
        "=",
        "=",
        "4",
        ".",
        "5",
        "3",
        ".",
        "2",
        " ",
        "\"",
        "h",
        "u",
        "g",
        "g",
        "i",
        "n",
        "g",
        "f",
        "a",
        "c",
        "e",
        "_",
        "h",
        "u",
        "b",
        ">",
        "=",
        "0",
        ".",
        "3",
        "4",
        ".",
        "0",
        "\"",
        " ",
        "\"",
        "d",
        "a",
        "t",
        "a",
        "s",
        "e",
        "t",
        "s",
        ">",
        "=",
        "3",
        ".",
        "4",
        ".",
        "1",
        ",",
        "<",
        "4",
        ".",
        "0",
        ".",
        "0",
        "\"",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# å¯¹äºæ›´å¿«çš„åŸ¹è®­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Mambaçš„CUDAåŠŸèƒ½\n",
        "!pip install --no-deps causal-conv1d==1.5.0.post8\n",
        "!pip install --no-build-isolation mamba-ssm==2.2.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ä¸å¡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.43.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c97875e799204d779e18fba0b95fd5bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.22G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c72c0615369427e962f09cc45097193",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab8732869fc3471ea8e0487873e48e15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/46.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6057012b69af430f804e9ef0fcc85965",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8e94a46887745b7b654aa12bc553ac1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a391fc6789a74ff3871c9dd840a72369",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # é€‰æ‹©ä»»ä½•ï¼æˆ‘ä»¬åœ¨å†…éƒ¨è‡ªåŠ¨æ”¯æŒç»³ç´¢æ‰©å±•ï¼\n",
        "dtype = (\n",
        "    None  # æ²¡æœ‰è‡ªåŠ¨æ£€æµ‹ã€‚ Tesla T4ï¼ŒV100ï¼ŒBfloat16çš„float16 for Ampere+\n",
        ")\n",
        "load_in_4bit = True  # ä½¿ç”¨4ä½é‡åŒ–æ¥å‡å°‘å†…å­˜ä½¿ç”¨é‡ã€‚å¯ä»¥æ˜¯é”™è¯¯çš„ã€‚\n",
        "\n",
        "# 4ä½é¢„é‡åŒ–æ¨¡å‹æˆ‘ä»¬æ”¯æŒ4å€ä¸‹è½½ + no oomsã€‚\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1 15ä¸‡äº¿ä»£å¸å‹2å€ï¼\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",  # æˆ‘ä»¬è¿˜ä¸Šä¼ äº†405Bçš„4ä½ï¼\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\",  # æ–°çš„Mistral 12b 2xå¿«ï¼\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",  # Mistral V3 2Xå¿«é€Ÿï¼\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",  # PHI-3 2å€ï¼\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",  # Gemma 2Xå¿«ï¼\n",
        "    \"unsloth/gemma-2-2b-bnb-4bit\",  # æ–°çš„å°å®çŸ³æ¨¡å‹ï¼\n",
        "]  # https://huggingface.co/unslothçš„æ›´å¤šå‹å·\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Falcon-H1-0.5B-Instruct\", # ä»https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8dfä¸­é€‰æ‹©ä»»ä½•å‹å·\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ç°åœ¨ï¼Œæˆ‘ä»¬æ·»åŠ Loraé€‚é…å™¨ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦æ›´æ–°æ‰€æœ‰å‚æ•°çš„1ï¼…è‡³10ï¼…ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.8 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # é€‰æ‹©ä»»ä½•æ•°å­—> 0ï¼å»ºè®®8ã€16ã€32ã€64ã€128\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"], # MAMBA OUT_PROJå’ŒCONV1Då±‚ä¸åº”åœ¨æ­¤å¤„åŒ…æ‹¬\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.1,\n",
        "    use_gradient_checkpointing = False,\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name =â€œ dataâ€> </a>\n",
        "###æ•°æ®å‡†å¤‡\n",
        "ç°åœ¨ï¼Œæˆ‘ä»¬ä½¿ç”¨XX_Markdown_link_xxçš„ç¾Šé©¼æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯åŸå§‹XX_Markdown_link_xxçš„52Kçš„è¿‡æ»¤ç‰ˆæœ¬ã€‚æ‚¨å¯ä»¥ç”¨è‡ªå·±çš„æ•°æ®å‡†å¤‡æ›¿æ¢æ­¤ä»£ç éƒ¨åˆ†ã€‚\n",
        "\n",
        "** [æ³¨æ„] **ä»…è®­ç»ƒå®Œæˆï¼ˆå¿½ç•¥ç”¨æˆ·çš„è¾“å…¥ï¼‰è¯»å–TRLçš„æ–‡æ¡£[yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned)ã€‚\n",
        "\n",
        "** [æ³¨æ„] **è¯·è®°ä½å°†** eos_token **æ·»åŠ åˆ°ä»¤ç‰ŒåŒ–è¾“å‡ºä¸­ï¼å¦åˆ™ï¼Œæ‚¨å°†è·å¾—æ— é™çš„å‡ ä»£äººï¼\n",
        "\n",
        "å¦‚æœæ‚¨æƒ³å°†`llama-3`æ¨¡æ¿ç”¨äºsharegptæ•°æ®é›†ï¼Œè¯·å°è¯•æˆ‘ä»¬çš„å¯¹è¯[Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html)-alpaca.ipynbï¼‰\n",
        "\n",
        "å¯¹äºè¯¸å¦‚æ–°é¢–å†™ä½œä¹‹ç±»çš„æ–‡æœ¬å®Œæˆï¼Œè¯·å°è¯•æ­¤[here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)-text_completion.ipynbï¼‰ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b65ab1fa061457e89f295e8a81e635b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b6fe8ee676c4dbeb48ee587c7be367a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a3ac24b2fbf45349c1a08a167a4fc8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "956ea575313b4306bed317ed3ea345eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "# ï¼ƒï¼ƒ æ“ä½œè¯´æ˜ï¼š\n",
        "{}\n",
        "\n",
        "# ï¼ƒï¼ƒ è¾“å…¥ï¼š\n",
        "{}\n",
        "\n",
        "# ï¼ƒï¼ƒ å›å¤ï¼š\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # å¿…é¡»æ·»åŠ eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # å¿…é¡»æ·»åŠ eos_tokenï¼Œå¦åˆ™æ‚¨è¿™ä¸€ä»£å°†æ°¸è¿œæŒç»­ä¸‹å»ï¼\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name =â€œ trainâ€> </a>\n",
        "###è®­ç»ƒæ¨¡å‹\n",
        "ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨huggingface trlçš„â€œ sfttrainerâ€ï¼è¿™é‡Œæ›´å¤šæ–‡æ¡£ï¼š[TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer)ã€‚æˆ‘ä»¬æ‰§è¡Œ60ä¸ªæ­¥éª¤æ¥åŠ å¿«é€Ÿåº¦ï¼Œä½†æ˜¯æ‚¨å¯ä»¥å°†`num_train_epochs = 1`è®¾ç½®ä¸ºå®Œæ•´è¿è¡Œï¼Œç„¶åå…³é—­`max_steps = noneâ€œã€‚æˆ‘ä»¬è¿˜æ”¯æŒTRLçš„â€œ Dpotrainerâ€ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92d9648dd6bb4086ad9ce52db641bf87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/51760 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # å¯ä»¥ä½¿çŸ­åºåˆ—æ›´å¿«åœ°è®­ç»ƒ5å€ã€‚\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        warmup_steps=5,\n",
        "        # num_train_epochs = 1ï¼Œï¼ƒå°†å…¶è®¾ç½®ä¸º1ä¸ªå®Œæ•´çš„è®­ç»ƒè¿è¡Œã€‚\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # å°†å…¶ç”¨äºWandbç­‰\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "2.697 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @Titleæ˜¾ç¤ºå½“å‰å†…å­˜ç»Ÿè®¡\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 20,766,720\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 03:16, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.854400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.406500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.755700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.990000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.632200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.651400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.220900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.359800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.127900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.266700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.028200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.990900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.156600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.968200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.961700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.075100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.302900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.028400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.925500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.960100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.975800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.900400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.033100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.105800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.113500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.090900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.939000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.883100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.941700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.914600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.920300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.024900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.868300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.964300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.908900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.908300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.806800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.139700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.216000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.963900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.984100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.945700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.923600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.974400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.971000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.925200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.234300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.932400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.085700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.062500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.971500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.244900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.856600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.071400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.925000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.845200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.895700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.958300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "278.0636 seconds used for training.\n",
            "4.63 minutes used for training.\n",
            "Peak reserved memory = 7.684 GB.\n",
            "Peak reserved memory for training = 4.987 GB.\n",
            "Peak reserved memory % of max memory = 52.102 %.\n",
            "Peak reserved memory for training % of max memory = 33.815 %.\n"
          ]
        }
      ],
      "source": [
        "# @Titleæ˜¾ç¤ºæœ€ç»ˆå†…å­˜å’Œæ—¶é—´ç»Ÿè®¡\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name =â€œæ¨ç†â€> </a>\n",
        "###æ¨ç†\n",
        "è®©æˆ‘ä»¬è¿è¡Œæ¨¡å‹ï¼æ‚¨å¯ä»¥æ›´æ”¹æŒ‡ä»¤å’Œè¾“å…¥ - å°†è¾“å‡ºç©ºç™½ä¿ç•™ï¼\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\nThe fibonnaci sequence is a sequence of numbers where each number is the sum of the two preceding ones. The sequence is defined as follows:\\n\\n1, 1, 2, 3, 5, 8, 13, 21, 34, 55, ']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ç¾ŠAlpaca_prompt =ä»ä¸Šæ–¹å¤åˆ¶\n",
        "FastLanguageModel.for_inference(model) # å¯ç”¨æœ¬åœ°2å€æ›´å¿«çš„æ¨ç†\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # æ“ä½œè¯´æ˜\n",
        "        \"1, 1, 2, 3, 5, 8\", # è¾“å…¥\n",
        "        \"\", # è¾“å‡º - å°†æ­¤ç©ºç™½ç•™ç»™ç”Ÿæˆï¼\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "æ‚¨è¿˜å¯ä»¥ä½¿ç”¨â€œ TextStreamerâ€è¿›è¡Œè¿ç»­æ¨ç† - å› æ­¤ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¤ç‰Œçœ‹åˆ°ä»£å¸ï¼Œè€Œä¸æ˜¯ä¸€ç›´åœ¨ç­‰å¾…ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "The fibonnaci sequence is a sequence of numbers where each number is the sum of the two preceding ones. The sequence is defined as follows:\n",
            "\n",
            "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 1771\n"
          ]
        }
      ],
      "source": [
        "# ç¾ŠAlpaca_prompt =ä»ä¸Šæ–¹å¤åˆ¶\n",
        "FastLanguageModel.for_inference(model) # å¯ç”¨æœ¬åœ°2å€æ›´å¿«çš„æ¨ç†\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Continue the fibonnaci sequence.\", # æ“ä½œè¯´æ˜\n",
        "        \"1, 1, 2, 3, 5, 8\", # è¾“å…¥\n",
        "        \"\", # è¾“å‡º - å°†æ­¤ç©ºç™½ç•™ç»™ç”Ÿæˆï¼\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name =â€œ saveâ€> </a>\n",
        "###ä¿å­˜ï¼ŒåŠ è½½å›ºå®šæ¨¡å‹\n",
        "è¦å°†æœ€ç»ˆæ¨¡å‹ä¿å­˜ä¸ºLoraé€‚é…å™¨ï¼Œè¯·ä½¿ç”¨HuggingFaceçš„â€œ push_to_hubâ€è¿›è¡Œåœ¨çº¿ä¿å­˜æˆ–`save_pretaining'ç”¨äºæœ¬åœ°ä¿å­˜ã€‚\n",
        "\n",
        "** [æ³¨æ„] **è¿™ä»…ä¿å­˜æ´›æ‹‰é€‚é…å™¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„æ¨¡å‹ã€‚è¦èŠ‚çœ16ä½æˆ–GGUFï¼Œè¯·å‘ä¸‹æ»šåŠ¨ï¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.model',\n",
              " 'lora_model/added_tokens.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # æœ¬åœ°èŠ‚çœ\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hubï¼ˆâ€œ your_name/lora_modelâ€ï¼Œtoken =â€œ ...â€ï¼‰ï¼ƒåœ¨çº¿ä¿å­˜\n",
        "# tokenizer.push_to_hubï¼ˆâ€œ your_name/lora_modelâ€ï¼Œtoken =â€œ ...â€ï¼‰ï¼ƒåœ¨çº¿ä¿å­˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ç°åœ¨ï¼Œå¦‚æœæ‚¨æƒ³åŠ è½½æ´›æ‹‰é€‚é…å™¨ï¼Œæˆ‘ä»¬åˆšåˆšä¿å­˜ç”¨äºæ¨ç†ï¼Œè¯·å°†`false``è®¾ç½®ä¸ºtrue`ï¼šï¼štrue'ï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is a famous tall tower in Paris?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The Eiffel Tower is a famous tall tower in Paris, France. It is located in the 5th arrondissement of Paris and is one of the most recognizable landmarks in the world. The tower was built for the 1889 World's Fair and is 324 meters tall. It is made of iron and has 1,665 steps. The tower is a symbol of Paris and is a popular tourist attraction.<eos>\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # æ‚¨ç”¨äºåŸ¹è®­çš„æ¨¡å‹\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # å¯ç”¨æœ¬åœ°2å€æ›´å¿«çš„æ¨ç†\n",
        "\n",
        "# ç¾ŠAlpaca_prompt =æ‚¨å¿…é¡»ä»ä¸Šæ–¹å¤åˆ¶ï¼\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"What is a famous tall tower in Paris?\", # æ“ä½œè¯´æ˜\n",
        "        \"\", # è¾“å…¥\n",
        "        \"\", # è¾“å‡º - å°†æ­¤ç©ºç™½ç•™ç»™ç”Ÿæˆï¼\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æ‹¥æŠ±è„¸çš„â€œ AutomodelForpeftCausAllmâ€ã€‚ä»…åœ¨æœªå®‰è£…``'''''æ—¶å°±ä½¿ç”¨ã€‚ç”±äºä¸æ”¯æŒâ€œ 4bitâ€æ¨¡å‹ä¸‹è½½ï¼Œå› æ­¤å®ƒå¯ä»¥æ…¢æ…¢æ”¾æ…¢ï¼Œå¹¶ä¸”ä¸èˆ’æœçš„**æ¨ç†å¿«2å€**ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if False:\n",
        "    # æˆ‘é«˜åº¦å»ºè®® - å¦‚æœå¯èƒ½çš„è¯ï¼Œè¯·ä½¿ç”¨ä¸å¡\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # æ‚¨ç”¨äºåŸ¹è®­çš„æ¨¡å‹\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### èŠ‚çœfloat16 vllm\n",
        "\n",
        "æˆ‘ä»¬è¿˜ç›´æ¥æ”¯æŒä¿å­˜åˆ°`float16`ã€‚ä¸ºfloat16é€‰æ‹©`merged_16bit`æˆ–ç”¨äºint4çš„merged_4bit`ã€‚æˆ‘ä»¬è¿˜å…è®¸`Lora`é€‚é…å™¨ä½œä¸ºåå¤‡ã€‚ä½¿ç”¨`push_to_hub_merged`ä¸Šä¼ åˆ°æ‚¨çš„æ‹¥æŠ±è„¸éƒ¨å¸æˆ·ï¼æ‚¨å¯ä»¥è®¿é—®https://huggingface.co/settings/tokens forä¸ªäººä»¤ç‰Œã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆå¹¶ä¸º16ä½\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# åˆå¹¶åˆ°4ä½\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# åªæ˜¯æ´›æ‹‰é€‚é…å™¨\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GGUF / LLAMA.CPPè½¬æ¢\n",
        "è¦ä¿å­˜åˆ°`gguf' /`llama.cpp`ï¼Œæˆ‘ä»¬ç°åœ¨åœ¨æœ¬åœ°æ”¯æŒå®ƒï¼æˆ‘ä»¬å…‹éš†â€œ llama.cppâ€ï¼Œé»˜è®¤å°†å…¶ä¿å­˜åˆ°Q8_0`ã€‚æˆ‘ä»¬å…è®¸æ‰€æœ‰æ–¹æ³•'q4_k_m`ã€‚ä½¿ç”¨`save_pretained_gguf`è¿›è¡Œæœ¬åœ°ä¿å­˜ï¼Œç„¶å`push_to_hub_gguf`å°†ä¸Šä¼ åˆ°HFã€‚\n",
        "\n",
        "ä¸€äº›æ”¯æŒçš„é‡åŒ–æ–¹æ³•ï¼ˆæˆ‘ä»¬çš„[Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)ä¸Šçš„å®Œæ•´åˆ—è¡¨ï¼‰ï¼š\n",
        "*`Q8_0`-å¿«é€Ÿè½¬æ¢ã€‚å¤§é‡èµ„æºä½¿ç”¨ï¼Œä½†é€šå¸¸å¯ä»¥æ¥å—ã€‚\n",
        "*`q4_k_m`-æ¨èã€‚ä½¿ç”¨q6_kä½œä¸ºæ³¨æ„åˆ°çš„ä¸€åŠã€‚\n",
        "*`q5_k_m`-æ¨èã€‚ä½¿ç”¨q6_kè¿›è¡Œä¸€åŠçš„æ³¨æ„ã€‚wvå’Œfeed_forward.w2å¼ é‡ï¼Œelse q5_kã€‚\n",
        "\n",
        "[** new **]è¦ä¸ºFinetuneå’Œè‡ªåŠ¨å¯¼å‡ºåˆ°Ollamaï¼Œè¯·å°è¯•æˆ‘ä»¬çš„[Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-olama.ipynbï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜åˆ°8ä½Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# è¯·è®°ä½è¦è®¿é—®https://huggingface.co/settings/tokens forä»£å¸ï¼\n",
        "# å¹¶å°†HFæ›´æ”¹ä¸ºæ‚¨çš„ç”¨æˆ·åï¼\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# ä¿å­˜è‡³16ä½GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# ä¿å­˜åˆ°Q4_K_M GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# ä¿å­˜åˆ°å¤šä¸ªGGUFé€‰é¡¹ - å¦‚æœè¦å¤šä¸ªï¼Œè¯·æ›´å¿«ï¼\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # å°†HFæ›´æ”¹ä¸ºæ‚¨çš„ç”¨æˆ·åï¼\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # åœ¨https://huggingface.co/settings/tokensä¸Šè·å–ä¸€ä¸ªä»¤ç‰Œ\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ç°åœ¨ï¼Œåœ¨llama.cppä¸­ä½¿ç”¨`model-unsloth.gguf`æ–‡ä»¶æˆ–`model-unsloth-q4_k_m.gguf`æ–‡ä»¶æˆ–åŸºäºUIçš„ç³»ç»Ÿï¼ˆä¾‹å¦‚JANæˆ–OPEN webUIï¼‰ã€‚æ‚¨å¯ä»¥å®‰è£…JAN XX_MARKDOWN_LINK_XXå¹¶æ‰“å¼€webui [here](https://github.com/janhq/jan)\n",
        "\n",
        "æˆ‘ä»¬å®Œæˆäº†ï¼å¦‚æœæ‚¨å¯¹ä¸èˆ’æœæœ‰ä»»ä½•ç–‘é—®ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªXX_MarkDown_link_xxé¢‘é“ï¼å¦‚æœæ‚¨å‘ç°ä»»ä½•é”™è¯¯æˆ–æƒ³å¯¹æœ€æ–°çš„LLMå†…å®¹è¿›è¡Œæ›´æ–°ï¼Œæˆ–è€…éœ€è¦å¸®åŠ©ï¼ŒåŠ å…¥é¡¹ç›®ç­‰ï¼Œè¯·éšæ—¶åŠ å…¥æˆ‘ä»¬çš„ä¸å’Œè°ï¼\n",
        "\n",
        "å…¶ä»–ä¸€äº›é“¾æ¥ï¼š\n",
        "1ã€‚è®­ç»ƒæ‚¨è‡ªå·±çš„æ¨ç†æ¨¡å‹-Llama grpoç¬”è®°æœ¬XX_Markdown_link_xx-grpo.ipynbï¼‰\n",
        "2ã€‚å°†èŠ¬å¤ªå°¼ä¿å­˜åˆ°å¥¥å°”é©¬ã€‚ [here](https://github.com/open-webui/open-webui)-ollama.ipynbï¼‰\n",
        "3ã€‚Llama3.2è§†è§‰ç‡ƒçƒ§ -  Xå°„çº¿ç…§ç›¸ç”¨ä¾‹ã€‚ [Discord](https://discord.gg/unsloth)-vision.ipynbï¼‰\n",
        "6ã€‚è¯·å‚è§DPOï¼ŒORPOï¼Œç»§ç»­é¢„å¤„ç†ï¼Œå¯¹è¯finetuningç­‰ç¬”è®°æœ¬ï¼Œä»¥åŠæˆ‘ä»¬çš„xx_markDown_link_xxä¸Šçš„æ›´å¤šå†…å®¹ï¼\n",
        "\n",
        "<div class =â€œ Align-Centerâ€>\n",
        "  <a href =â€œ https://unsloth.aiâ€> <img src =â€œ https://github.com/unslothai/unslothai/unsloth/raw/raw/main/main/images/unsloth%20new%20new%20logo.pngâ€\n",
        "  <a href =â€œ https://discord.gg/unslothâ€> <img src =â€œ https://github.com/unslothai/unsloth/unsloth/raw/main/main/main/images/images/discord.pngï¼Œpng\n",
        "  <a href =â€œ https://docs.unsloth.ai/â€> <img src =â€œ https://github.com/unslothai/unslothai/unsloth/unsloth/main/main/main/images/images/images/documentation%20green%20green%20breen%20button.png?png?png?raw=true width =â€\n",
        "\n",
        "  å¦‚æœæ‚¨éœ€è¦å¸®åŠ©ï¼Œè¯·åŠ å…¥DISCORD +â­ï¸<i>åœ¨<a href =â€œ https://github.com/unslothai/unslothä¸Šâ€\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}