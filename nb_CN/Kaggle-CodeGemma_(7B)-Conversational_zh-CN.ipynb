{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要运行此操作，请按“*Runtime*”，然后按“*运行*” ** free ** tesla Tesla T4 Google COLAB实例！\n",
        "<div class =“ Align-Center”>\n",
        "<a href =“ https://unsloth.ai/”> <img src =“ https://github.com/unslothai/unslothai/unsloth/raw/raw/main/main/mains/unsloth%20new%20new%20logo.png.png”\n",
        "<a href =“ https://discord.gg/unsloth”> <img src =“ https://github.com/unslothai/unslothai/unsloth/raw/main/main/main/images/images/discord button.png button.png\n",
        "<a href =“ https://docs.unsloth.ai/”> <img src =“ https://github.com/unslothai/unslothai/unsloth/unsloth/main/main/main/mains/images/documentation%20green%20green%20breen%20button.png?png?raw=raw=true width width” <i>在<a href =“ https://github.com/unslothai/unsloth”> github </a> </i>⭐中\n",
        "</div>\n",
        "\n",
        "要在您自己的计算机上安装不塞，请按照我们的github页面上的安装说明[here](https://docs.unsloth.ai/get-started/installing-+-updating)上的安装说明。\n",
        "\n",
        "您将学习如何做[data prep](#Data)，如何[train](#Train)，如何[run the model](#Inference)，＆xx_markDown_link_xx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 消息"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "** New ** Unsploth现在支持培训OpenAi的新** GPT-oss **模型！您可以通过我们的** [Colab notebook](https://x.com/UnslothAI/status/1953896997867729075)免费启动Finetune GPT-oss **！\n",
        "\n",
        "Unsploth现在支持文本对语音（TTS）模型。阅读我们的xx_markDown_link_xx。\n",
        "\n",
        "阅读我们的** [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning) **并查看我们的新** [Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune) ** Quants优先量优于其他量化方法！\n",
        "\n",
        "请访问我们的所有文档，以获取我们的所有XX_MarkDown_link_xx和[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%",
        "%",
        "c",
        "a",
        "p",
        "t",
        "u",
        "r",
        "e",
        "\n",
        "i",
        "m",
        "p",
        "o",
        "r",
        "t",
        " ",
        "o",
        "s",
        "\n",
        "o",
        "s",
        ".",
        "e",
        "n",
        "v",
        "i",
        "r",
        "o",
        "n",
        "[",
        "\"",
        "C",
        "U",
        "D",
        "A",
        "_",
        "V",
        "I",
        "S",
        "I",
        "B",
        "L",
        "E",
        "_",
        "D",
        "E",
        "V",
        "I",
        "C",
        "E",
        "S",
        "\"",
        "]",
        " ",
        "=",
        " ",
        "\"",
        "0",
        "\"",
        "\n",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "p",
        "i",
        "p",
        "3",
        "-",
        "a",
        "u",
        "t",
        "o",
        "r",
        "e",
        "m",
        "o",
        "v",
        "e",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "t",
        "o",
        "r",
        "c",
        "h",
        " ",
        "t",
        "o",
        "r",
        "c",
        "h",
        "v",
        "i",
        "s",
        "i",
        "o",
        "n",
        " ",
        "t",
        "o",
        "r",
        "c",
        "h",
        "a",
        "u",
        "d",
        "i",
        "o",
        " ",
        "x",
        "f",
        "o",
        "r",
        "m",
        "e",
        "r",
        "s",
        " ",
        "-",
        "-",
        "i",
        "n",
        "d",
        "e",
        "x",
        "-",
        "u",
        "r",
        "l",
        " ",
        "h",
        "t",
        "t",
        "p",
        "s",
        ":",
        "/",
        "/",
        "d",
        "o",
        "w",
        "n",
        "l",
        "o",
        "a",
        "d",
        ".",
        "p",
        "y",
        "t",
        "o",
        "r",
        "c",
        "h",
        ".",
        "o",
        "r",
        "g",
        "/",
        "w",
        "h",
        "l",
        "/",
        "c",
        "u",
        "1",
        "2",
        "4",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "u",
        "n",
        "s",
        "l",
        "o",
        "t",
        "h",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "-",
        "-",
        "u",
        "p",
        "g",
        "r",
        "a",
        "d",
        "e",
        " ",
        "t",
        "r",
        "a",
        "n",
        "s",
        "f",
        "o",
        "r",
        "m",
        "e",
        "r",
        "s",
        "=",
        "=",
        "4",
        ".",
        "5",
        "3",
        ".",
        "2",
        " ",
        "\"",
        "h",
        "u",
        "g",
        "g",
        "i",
        "n",
        "g",
        "f",
        "a",
        "c",
        "e",
        "_",
        "h",
        "u",
        "b",
        ">",
        "=",
        "0",
        ".",
        "3",
        "4",
        ".",
        "0",
        "\"",
        " ",
        "\"",
        "d",
        "a",
        "t",
        "a",
        "s",
        "e",
        "t",
        "s",
        ">",
        "=",
        "3",
        ".",
        "4",
        ".",
        "1",
        ",",
        "<",
        "4",
        ".",
        "0",
        ".",
        "0",
        "\"",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 不塞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "3824f024-13a6-40d6-f30f-03e9161d0733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Gemma patching release 2024.4\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 4096  # 杰玛可悲的是，目前只支持最大8192\n",
        "dtype = (\n",
        "    None  # 没有自动检测。 Tesla T4，V100，Bfloat16的float16 for Ampere+\n",
        ")\n",
        "load_in_4bit = True  # 使用4位量化来减少内存使用量。可以是错误的。\n",
        "\n",
        "# 4位预量化模型我们支持4倍下载 + no ooms。\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "    \"unsloth/codellama-34b-bnb-4bit\",\n",
        "    \"unsloth/tinyllama-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",  # 新的Google 6万亿代币2.5倍！\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "]  # https://huggingface.co/unsloth的更多型号\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/codegemma-7b-bnb-4bit\",  # 选择任何！例如Teknium/OpenHermes-2.5-Mistral-7b\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token =“ hf _...”，＃如果使用元模型，例如meta-llama/llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "现在，我们添加Lora适配器，因此我们只需要更新所有参数的1％至10％！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "0f03fa94-1657-4ee9-a505-47b02a57e3c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # 选择任何数字> 0！建议8、16、32、64、128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # 支持任何人，但= 0已优化\n",
        "    bias = \"none\",    # 支持任何人，但是=“无”\n",
        "    use_gradient_checkpointing = \"unsloth\", # 在很长的背景下真实或“不整齐”\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # 我们支持排名稳定的洛拉\n",
        "    loftq_config = None, # 和Loftq\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name =“ data”> </a>\n",
        "###数据准备\n",
        "现在，我们将“ ChatMl”格式用于对话风格的Finetunes。我们在ShareGpt样式中使用xx_markDown_link_xx。 CHATML渲染多转向对话如下：\n",
        "\n",
        "```\n",
        "<|im_start|>system\n",
        "You are a helpful assistant.<|im_end|>\n",
        "<|im_start|>user\n",
        "What's the capital of France?<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Paris.\n",
        "```\n",
        "\n",
        "** [注意] **仅训练完成（忽略用户的输入）读取TRL的文档[Open Assistant conversations](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style)。\n",
        "\n",
        "我们使用“ get_chat_template”函数获取正确的聊天模板。我们支持“ Zephyr，Chatml，Mistral，Llama，羊驼，Vicuna，Vicuna_old”和我们自己的优化`'Unsploth'模板。\n",
        "\n",
        "通常，一个人必须训练`<| im_start |>`和`<| im_end |>`。相反，我们将`<| im_end |>`作为eos代币，然后按原样留下`<| im_start |>`。这不需要对其他令牌进行额外的培训。\n",
        "\n",
        "Note ShareGpt使用`{“ from”：“ human”，“ value”：“ hi”}`而不是`{“ cole”：“ user”：“ user”，“ content”：“ hi”}`，因此我们使用`映射'将其映射。\n",
        "\n",
        "对于诸如新颖写作之类的文本完成，请尝试此[here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)-text_completion.ipynb）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # 支持Zephyr，Chatml，Mistral，Llama，羊驼，Vicuna，Vicuna_old，Unsploth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT风格\n",
        "    map_eos_token = True, # 地图<| im_end |> to </s>\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHiVoToneynS"
      },
      "source": [
        "让我们看看“ chatml”格式如何通过打印第5个元素来工作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GSuKSSbpYKq",
        "outputId": "2bc9b5a8-70c0-4924-b456-0138442031bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'from': 'human',\n",
              "  'value': 'What is the typical wattage of bulb in a lightbox?'},\n",
              " {'from': 'gpt',\n",
              "  'value': 'The typical wattage of a bulb in a lightbox is 60 watts, although domestic LED bulbs are normally much lower than 60 watts, as they produce the same or greater lumens for less wattage than alternatives. A 60-watt Equivalent LED bulb can be calculated using the 7:1 ratio, which divides 60 watts by 7 to get roughly 9 watts.'},\n",
              " {'from': 'human',\n",
              "  'value': 'Rewrite your description of the typical wattage of a bulb in a lightbox to only include the key points in a list format.'}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5iEWrUkevpE",
        "outputId": "4f6de3db-4c48-41b1-e505-f4efb1674a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos><|im_start|>user\n",
            "What is the typical wattage of bulb in a lightbox?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "The typical wattage of a bulb in a lightbox is 60 watts, although domestic LED bulbs are normally much lower than 60 watts, as they produce the same or greater lumens for less wattage than alternatives. A 60-watt Equivalent LED bulb can be calculated using the 7:1 ratio, which divides 60 watts by 7 to get roughly 9 watts.<|im_end|>\n",
            "<|im_start|>user\n",
            "Rewrite your description of the typical wattage of a bulb in a lightbox to only include the key points in a list format.<|im_end|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(dataset[5][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKOAUDpUeDL"
      },
      "source": [
        "如果您想制作自己的聊天模板，那也是可能的！您必须使用Jinja模板制度。我们提供了自己的“不塞模板”的剥离版本，我们发现它更有效，并利用了Chatml，Zephyr和羊驼风格。\n",
        "\n",
        "XX_Markdown_link_xx上的聊天模板上的更多信息"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p31Z-S6FUieB"
      },
      "outputs": [],
      "source": [
        "unsloth_template = \\\n",
        "    \"{{ bos_token }}\"\\\n",
        "    \"{{ 'You are a helpful assistant to the user\\n' }}\"\\\n",
        "    \"{% endif %}\"\\\n",
        "    \"{% for message in messages %}\"\\\n",
        "        \"{% if message['role'] == 'user' %}\"\\\n",
        "            \"{{ '>>> User: ' + message['content'] + '\\n' }}\"\\\n",
        "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
        "            \"{{ '>>> Assistant: ' + message['content'] + eos_token + '\\n' }}\"\\\n",
        "        \"{% endif %}\"\\\n",
        "    \"{% endfor %}\"\\\n",
        "    \"{% if add_generation_prompt %}\"\\\n",
        "        \"{{ '>>> Assistant: ' }}\"\\\n",
        "    \"{% endif %}\"\n",
        "unsloth_eos_token = \"eos_token\"\n",
        "\n",
        "if False:\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = (unsloth_template, unsloth_eos_token,), # 您必须提供模板和EOS令牌\n",
        "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT风格\n",
        "        map_eos_token = True, # 地图<| im_end |> to </s>\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name =“ train”> </a>\n",
        "###训练模型\n",
        "现在，让我们使用huggingface trl的“ sfttrainer”！这里更多文档：[TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer)。我们执行60个步骤来加快速度，但是您可以将`num_train_epochs = 1`设置为完整运行，然后关闭`max_steps = none“。我们还支持TRL的“ Dpotrainer”！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    packing = False,  # 可以使短序列更快地训练5倍。\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 20,\n",
        "        learning_rate = 2e-4,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        dataset_text_field = \"text\",\n",
        "        report_to = \"none\",  # 将其用于Wandb等\n",
        "        max_grad_norm = 0.3,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "27e0dbb5-f81e-445b-8d61-75f1c2b2b73a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "5.967 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @Title显示当前内存统计\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "d92834e7-9463-4040-94e6-672243709216"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 9,033 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 4 | Total steps = 20\n",
            " \"-____-\"     Number of trainable parameters = 50,003,968\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 02:33, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.941400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.585900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.614300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.335900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.123000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.364300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.392600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.775400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.273400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.502900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.802700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.099600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.497100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.371100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.617200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.402300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.746100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.621100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.919900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "90cc348e-4418-4ecd-faf6-cd37595a86d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "164.0222 seconds used for training.\n",
            "2.73 minutes used for training.\n",
            "Peak reserved memory = 12.316 GB.\n",
            "Peak reserved memory for training = 6.349 GB.\n",
            "Peak reserved memory % of max memory = 83.51 %.\n",
            "Peak reserved memory for training % of max memory = 43.05 %.\n"
          ]
        }
      ],
      "source": [
        "# @Title显示最终内存和时间统计\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name =“推理”> </a>\n",
        "###推理\n",
        "让我们运行模型！由于我们正在使用`chatml'，因此将``apply_chat_template''使用`add_generation_prompt`设置为true`进行推理。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "8e932364-849a-40ad-e0b5-5f5ffd028f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|> is already a token. Skipping.\n",
            "<|im_end|> is already a token. Skipping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<bos><|im_start|>user\\nContinue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|im_end|>\\n<|im_start|>assistant\\nThe next number in the Fibonacci sequence is 13.<|im_end|>']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", # 支持Zephyr，Chatml，Mistral，Llama，羊驼，Vicuna，Vicuna_old，Unsploth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT风格\n",
        "    map_eos_token = True, # 地图<| im_end |> to </s>\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # 启用本地2倍更快的推理\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # 必须添加一代\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        "您还可以使用“ TextStreamer”进行连续推理 - 因此，您可以通过令牌看到代币，而不是一直在等待！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "6b9ace79-d6f3-41aa-9e04-879506f14f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos><|im_start|>user\n",
            "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|im_end|>\n",
            "<|im_start|>assistant\n",
            "The next number in the Fibonacci sequence is 13.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # 启用本地2倍更快的推理\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # 必须添加一代\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name =“ save”> </a>\n",
        "###保存，加载固定模型\n",
        "要将最终模型保存为Lora适配器，请使用HuggingFace的“ push_to_hub”进行在线保存或`save_pretaining'用于本地保存。\n",
        "\n",
        "** [注意] **这仅保存洛拉适配器，而不是完整的模型。要节省16位或GGUF，请向下滚动！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # 本地节省\n",
        "# model.push_to_hub（“ your_name/lora_model”，token =“ ...”）＃在线保存"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "现在，如果您想加载洛拉适配器，我们刚刚保存用于推理，请将`false``设置为true`：：true'："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "e8cff9c9-3e85-46ea-9d6a-735935fc88a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos><|im_start|>user\n",
            "What is a famous tall tower in Paris?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "The Eiffel Tower is a famous tall tower in Paris. It was built in 1889 to commemorate the 100th anniversary of the French Revolution. It is 324 meters tall and is one of the most recognizable landmarks in the world.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # 您用于培训的模型\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # 启用本地2倍更快的推理\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # 必须添加一代\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "您也可以使用拥抱脸的“ AutomodelForpeftCausAllm”。仅在未安装``'''''时就使用。由于不支持“ 4bit”模型下载，因此它可以慢慢放慢，并且不舒服的**推理快2倍**。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # 我高度建议 - 如果可能的话，请使用不塞\n",
        "    from peft import AutoModelForPeftCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    model = AutoModelForPeftCausalLM.from_pretrained(\n",
        "        \"lora_model\",  # 您用于培训的模型\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### 节省float16 vllm\n",
        "\n",
        "我们还直接支持保存到`float16`。为float16选择`merged_16bit`或用于int4的merged_4bit`。我们还允许`Lora`适配器作为后备。使用`push_to_hub_merged`上传到您的拥抱脸部帐户！您可以访问https://huggingface.co/settings/tokens for个人令牌。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# 合并为16位\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# 合并到4位\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# 只是洛拉适配器\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / LLAMA.CPP转换\n",
        "要保存到`gguf' /`llama.cpp`，我们现在在本地支持它！我们克隆“ llama.cpp”，默认将其保存到Q8_0`。我们允许所有方法'q4_k_m`。使用`save_pretained_gguf`进行本地保存，然后`push_to_hub_gguf`将上传到HF。\n",
        "\n",
        "一些支持的量化方法（我们的[Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)上的完整列表）：\n",
        "*`Q8_0`-快速转换。大量资源使用，但通常可以接受。\n",
        "*`q4_k_m`-推荐。使用q6_k作为注意到的一半。\n",
        "*`q5_k_m`-推荐。使用q6_k进行一半的注意。wv和feed_forward.w2张量，else q5_k。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# 保存到8位Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# 保存至16位GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# 保存到Q4_K_M GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在，在llama.cpp中使用`model-unsloth.gguf`文件或`model-unsloth-q4_k_m.gguf`文件或基于UI的系统（例如JAN或OPEN webUI）。您可以安装JAN XX_MARKDOWN_LINK_XX并打开webui [here](https://github.com/janhq/jan)\n",
        "\n",
        "我们完成了！如果您对不舒服有任何疑问，我们有一个XX_MarkDown_link_xx频道！如果您发现任何错误或想对最新的LLM内容进行更新，或者需要帮助，加入项目等，请随时加入我们的不和谐！\n",
        "\n",
        "其他一些链接：\n",
        "1。训练您自己的推理模型-Llama grpo笔记本XX_Markdown_link_xx-grpo.ipynb）\n",
        "2。将芬太尼保存到奥尔马。 [here](https://github.com/open-webui/open-webui)-ollama.ipynb）\n",
        "3。Llama3.2视觉燃烧 -  X射线照相用例。 [Discord](https://discord.gg/unsloth)-vision.ipynb）\n",
        "6。请参见DPO，ORPO，继续预处理，对话finetuning等笔记本，以及我们的xx_markDown_link_xx上的更多内容！\n",
        "\n",
        "<div class =“ Align-Center”>\n",
        "  <a href =“ https://unsloth.ai”> <img src =“ https://github.com/unslothai/unslothai/unsloth/raw/raw/main/main/images/unsloth%20new%20new%20logo.png”\n",
        "  <a href =“ https://discord.gg/unsloth”> <img src =“ https://github.com/unslothai/unsloth/unsloth/raw/main/main/main/images/images/discord.png，png\n",
        "  <a href =“ https://docs.unsloth.ai/”> <img src =“ https://github.com/unslothai/unslothai/unsloth/unsloth/main/main/main/images/images/images/documentation%20green%20green%20breen%20button.png?png?png?raw=true width =”\n",
        "\n",
        "  如果您需要帮助，请加入DISCORD +⭐️<i>在<a href =“ https://github.com/unslothai/unsloth上”\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}