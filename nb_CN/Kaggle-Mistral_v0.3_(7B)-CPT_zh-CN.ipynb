{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要运行此操作，请按“*Runtime*”，然后按“*运行*” ** free ** tesla Tesla T4 Google COLAB实例！\n",
        "<div class =“ Align-Center”>\n",
        "<a href =“ https://unsloth.ai/”> <img src =“ https://github.com/unslothai/unslothai/unsloth/raw/raw/main/main/mains/unsloth%20new%20new%20logo.png.png”\n",
        "<a href =“ https://discord.gg/unsloth”> <img src =“ https://github.com/unslothai/unslothai/unsloth/raw/main/main/main/images/images/discord button.png button.png\n",
        "<a href =“ https://docs.unsloth.ai/”> <img src =“ https://github.com/unslothai/unslothai/unsloth/unsloth/main/main/main/mains/images/documentation%20green%20green%20breen%20button.png?png?raw=raw=true width width” <i>在<a href =“ https://github.com/unslothai/unsloth”> github </a> </i>⭐中\n",
        "</div>\n",
        "\n",
        "要在您自己的计算机上安装不塞，请按照我们的github页面上的安装说明[here](https://docs.unsloth.ai/get-started/installing-+-updating)上的安装说明。\n",
        "\n",
        "您将学习如何做[data prep](#Data)，如何[train](#Train)，如何[run the model](#Inference)，＆xx_markDown_link_xx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 消息"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "** New ** Unsploth现在支持培训OpenAi的新** GPT-oss **模型！您可以通过我们的** [Colab notebook](https://x.com/UnslothAI/status/1953896997867729075)免费启动Finetune GPT-oss **！\n",
        "\n",
        "Unsploth现在支持文本对语音（TTS）模型。阅读我们的xx_markDown_link_xx。\n",
        "\n",
        "阅读我们的** [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning) **并查看我们的新** [Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune) ** Quants优先量优于其他量化方法！\n",
        "\n",
        "请访问我们的所有文档，以获取我们的所有XX_MarkDown_link_xx和[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%",
        "%",
        "c",
        "a",
        "p",
        "t",
        "u",
        "r",
        "e",
        "\n",
        "i",
        "m",
        "p",
        "o",
        "r",
        "t",
        " ",
        "o",
        "s",
        "\n",
        "o",
        "s",
        ".",
        "e",
        "n",
        "v",
        "i",
        "r",
        "o",
        "n",
        "[",
        "\"",
        "C",
        "U",
        "D",
        "A",
        "_",
        "V",
        "I",
        "S",
        "I",
        "B",
        "L",
        "E",
        "_",
        "D",
        "E",
        "V",
        "I",
        "C",
        "E",
        "S",
        "\"",
        "]",
        " ",
        "=",
        " ",
        "\"",
        "0",
        "\"",
        "\n",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "p",
        "i",
        "p",
        "3",
        "-",
        "a",
        "u",
        "t",
        "o",
        "r",
        "e",
        "m",
        "o",
        "v",
        "e",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "t",
        "o",
        "r",
        "c",
        "h",
        " ",
        "t",
        "o",
        "r",
        "c",
        "h",
        "v",
        "i",
        "s",
        "i",
        "o",
        "n",
        " ",
        "t",
        "o",
        "r",
        "c",
        "h",
        "a",
        "u",
        "d",
        "i",
        "o",
        " ",
        "x",
        "f",
        "o",
        "r",
        "m",
        "e",
        "r",
        "s",
        " ",
        "-",
        "-",
        "i",
        "n",
        "d",
        "e",
        "x",
        "-",
        "u",
        "r",
        "l",
        " ",
        "h",
        "t",
        "t",
        "p",
        "s",
        ":",
        "/",
        "/",
        "d",
        "o",
        "w",
        "n",
        "l",
        "o",
        "a",
        "d",
        ".",
        "p",
        "y",
        "t",
        "o",
        "r",
        "c",
        "h",
        ".",
        "o",
        "r",
        "g",
        "/",
        "w",
        "h",
        "l",
        "/",
        "c",
        "u",
        "1",
        "2",
        "4",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "u",
        "n",
        "s",
        "l",
        "o",
        "t",
        "h",
        "\n",
        "!",
        "p",
        "i",
        "p",
        " ",
        "i",
        "n",
        "s",
        "t",
        "a",
        "l",
        "l",
        " ",
        "-",
        "-",
        "u",
        "p",
        "g",
        "r",
        "a",
        "d",
        "e",
        " ",
        "t",
        "r",
        "a",
        "n",
        "s",
        "f",
        "o",
        "r",
        "m",
        "e",
        "r",
        "s",
        "=",
        "=",
        "4",
        ".",
        "5",
        "3",
        ".",
        "2",
        " ",
        "\"",
        "h",
        "u",
        "g",
        "g",
        "i",
        "n",
        "g",
        "f",
        "a",
        "c",
        "e",
        "_",
        "h",
        "u",
        "b",
        ">",
        "=",
        "0",
        ".",
        "3",
        "4",
        ".",
        "0",
        "\"",
        " ",
        "\"",
        "d",
        "a",
        "t",
        "a",
        "s",
        "e",
        "t",
        "s",
        ">",
        "=",
        "3",
        ".",
        "4",
        ".",
        "1",
        ",",
        "<",
        "4",
        ".",
        "0",
        ".",
        "0",
        "\"",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 不塞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%env UNSLOTH_RETURN_LOGITS=1 # 将其运行以禁用CCE，因为它不支持CPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # 选择任何！我们在内部自动支持绳索扩展！\n",
        "dtype = None # 没有自动检测。 Tesla T4，V100，Bfloat16的float16 for Ampere+\n",
        "load_in_4bit = True # 使用4位量化来减少内存使用量。可以是错误的。\n",
        "\n",
        "# 4位预量化模型我们支持4倍下载 + no ooms。\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # 新的Mistral V3 2X快速！\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3-15万亿代币2型快速！\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # PHI-3 2倍！\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2倍！\n",
        "] # https://huggingface.co/unsloth的更多型号\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-v0.3\", # 选择任何！例如Teknium/OpenHermes-2.5-Mistral-7b\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token =“ hf _...”，＃如果使用元模型，例如meta-llama/llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "现在，我们添加Lora适配器，因此我们只需要更新所有参数的1％至10％！\n",
        "\n",
        "我们还添加了`embed_tokens'和`lm_head''，以允许模型从分发数据中学习。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # 选择任何数字> 0！建议8、16、32、64、128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "\n",
        "                      \"embed_tokens\", \"lm_head\",], # 添加以持续预处理\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # 支持任何人，但= 0已优化\n",
        "    bias = \"none\",    # 支持任何人，但是=“无”\n",
        "    # [NEW]“ Unsploth”使用的VRAM减少了30％，适合2倍批量的尺寸！\n",
        "    use_gradient_checkpointing = \"unsloth\", # 在很长的背景下真实或“不整齐”\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,   # 我们支持排名稳定的洛拉\n",
        "    loftq_config = None, # 和Loftq\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name =“ data”> </a>\n",
        "###数据准备\n",
        "现在，我们使用XX_MarkDown_link_xx的韩国子集首先不断预认识该模型。您可以使用**您喜欢的任何语言**！转到[Wikipedia dataset](https://huggingface.co/datasets/wikimedia/wikipedia)查找自己的语言！\n",
        "\n",
        "** [注意] **仅训练完成（忽略用户的输入）读取TRL的文档[Wikipedia's List of Languages](https://en.wikipedia.org/wiki/List_of_Wikipedias)。\n",
        "\n",
        "** [注意] **请记住将** eos_token **添加到令牌化输出中！否则，您将获得无限的几代人！\n",
        "\n",
        "如果您想将`llama-3`模板用于sharegpt数据集，请尝试我们的对话[here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)-conversations.ipynb）\n",
        "\n",
        "对于诸如新颖写作之类的文本完成，请尝试此[notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-text_completion.ipynb）。\n",
        "\n",
        "** [注意] **使用https://translate.google.com从英语翻译成韩文！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "# Wikipedia提供了标题和文章文本。\n",
        "# 使用https://translate.google.com！\n",
        "_wikipedia_prompt = \"\"\"Wikipedia Article\n",
        "# ＃＃ 标题： {}\n",
        "\n",
        "# ＃＃ 文章：\n",
        "{}\"\"\"\n",
        "# 变成：\n",
        "wikipedia_prompt = \"\"\"위키피디아 기사\n",
        "# ＃＃ 标题： {}\n",
        "\n",
        "# ＃＃ 文章：\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # 必须添加eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    titles = examples[\"title\"]\n",
        "    texts  = examples[\"text\"]\n",
        "    outputs = []\n",
        "    for title, text in zip(titles, texts):\n",
        "        # 必须添加eos_token，否则您这一代将永远持续下去！\n",
        "        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n",
        "        outputs.append(text)\n",
        "    return { \"text\" : outputs, }\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw0tuebw4Ppe"
      },
      "source": [
        "我们只使用1％的数据集来加快速度！用于长期运行！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsKrpkza3VB3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.ko\", split = \"train\",)\n",
        "\n",
        "# 我们选择1％的数据以使培训更快！\n",
        "dataset = dataset.train_test_split(train_size = 0.01)[\"train\"]\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name =“ train”> </a>\n",
        "###继续预处理\n",
        "现在，让我们使用Unsploth的“ unsplostrainer”！这里更多文档：[TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer)。我们执行20个步骤来加快速度，但是您可以将`num_train_epochs = 1`设置为完整运行，然后关闭`max_steps = none“。\n",
        "\n",
        "还将`embedding_learning_rate`设置为学习率至少比“ Learning_rate”小2倍或10倍，以进行持续的预处理！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 4,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "\n",
        "        # 使用Hallup_ratio和num_train_epochs进行较长的运行！\n",
        "        max_steps = 120,\n",
        "        warmup_steps = 10,\n",
        "        # 热身= 0.1，\n",
        "        # num_train_epochs = 1，\n",
        "\n",
        "        # 为嵌入矩阵选择2至10倍的学习率！\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # 将其用于Wandb等\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @Title显示当前内存统计\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9dRBJZulavZ"
      },
      "source": [
        "### 指令填充\n",
        "\n",
        "现在，我们使用xx_markDown_link_xx，但在韩文中进行了翻译！\n",
        "\n",
        "对于羊驼或XX_Markdown_link_xx的原始GPT4数据集，请访问XX_Markdown_link_xx，以获取羊Alpaca数据集的其他翻译。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oNjUwxOyG8C"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "alpaca_dataset = load_dataset(\"FreedomIntelligence/alpaca-gpt4-korean\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmFG41nFytgi"
      },
      "source": [
        "我们打印1个示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvTfIKaUQxQ5"
      },
      "outputs": [],
      "source": [
        "print(alpaca_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO8UY34ql2vJ"
      },
      "source": [
        "我们再次使用https://translate.google.com/将羊驼格式翻译成韩文"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2tv8AEhPTu6"
      },
      "outputs": [],
      "source": [
        "_alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "# ＃＃ 操作说明：\n",
        "{}\n",
        "\n",
        "# ＃＃ 回复：\n",
        "{}\"\"\"\n",
        "# 变成：\n",
        "alpaca_prompt = \"\"\"다음은 작업을 설명하는 명령입니다. 요청을 적절하게 완료하는 응답을 작성하세요.\n",
        "\n",
        "# ##指南：\n",
        "{}\n",
        "\n",
        "# ＃＃ 回复：\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # 必须添加eos_token\n",
        "def formatting_prompts_func(conversations):\n",
        "    texts = []\n",
        "    conversations = conversations[\"conversations\"]\n",
        "    for convo in conversations:\n",
        "        # 必须添加eos_token，否则您这一代将永远持续下去！\n",
        "        text = alpaca_prompt.format(convo[0][\"value\"], convo[1][\"value\"]) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "alpaca_dataset = alpaca_dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxWWh8xsl9XT"
      },
      "source": [
        "我们再次采用“不舒服”并进行指导填充！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Zul21NSRRLP"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = alpaca_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 8,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "\n",
        "        # 使用num_train_epochs和Hallup_ratio进行长期运行！\n",
        "        max_steps = 120,\n",
        "        warmup_steps = 10,\n",
        "        # 热身= 0.1，\n",
        "        # num_train_epochs = 1，\n",
        "\n",
        "        # 为嵌入矩阵选择2至10倍的学习率！\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.00,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # 将其用于Wandb等\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIO7c1FoRe-X"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @Title显示最终内存和时间统计\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name =“推理”> </a>\n",
        "###推理\n",
        "让我们运行模型！您可以更改指令和输入 - 将输出空白保留！\n",
        "\n",
        "切记使用https://translate.google.com/！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_GRpqmUiFRj"
      },
      "outputs": [],
      "source": [
        "# 羊Alpaca_prompt =从上方复制\n",
        "FastLanguageModel.for_inference(model) # 启用本地2倍更快的推理\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        # “继续斐波那契序列：1、1、2、3、5、8，”，＃指令\n",
        "        \"피보나치 수열을 계속하세요: 1, 1, 2, 3, 5, 8,\", # 操作说明\n",
        "        \"\", # 输出 - 将此空白留给生成！\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        "您还可以使用“ TextStreamer”进行连续推理 - 因此，您可以通过令牌看到代币，而不是一直在等待！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "# 羊Alpaca_prompt =从上方复制\n",
        "FastLanguageModel.for_inference(model) # 启用本地2倍更快的推理\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        # “韩国音乐是什么样的？”\n",
        "        \"한국음악은 어떤가요?\", # 操作说明\n",
        "        \"\", # 输出 - 将此空白留给生成！\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acKgU7NMauCp"
      },
      "source": [
        "通过使用https://translate.google.com/我们得到\n",
        "```\n",
        "Korean music is classified into many types of music genres.\n",
        "\n",
        "This genre is classified into different music genres such as pop songs,\n",
        "\n",
        "rock songs, classical songs and pop songs, music groups consisting of drums, fans, instruments and singers\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name =“ save”> </a>\n",
        "###保存，加载固定模型\n",
        "要将最终模型保存为Lora适配器，请使用HuggingFace的“ push_to_hub”进行在线保存或`save_pretaining'用于本地保存。\n",
        "\n",
        "** [注意] **这仅保存洛拉适配器，而不是完整的模型。要节省16位或GGUF，请向下滚动！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # 本地节省\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub（“ your_name/lora_model”，token =“ ...”）＃在线保存\n",
        "# tokenizer.push_to_hub（“ your_name/lora_model”，token =“ ...”）＃在线保存"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "现在，如果您想加载洛拉适配器，我们刚刚保存用于推理，请将`false``设置为true`：：true'："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"lora_model\",  # 您用于培训的模型\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=dtype,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)  # 启用本地2倍更快的推理\n",
        "\n",
        "# 羊Alpaca_prompt =您必须从上方复制！\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            # “广泛描述地球。”，＃指示\n",
        "            \"지구를 광범위하게 설명하세요.\",\n",
        "            \"\",  # 输出 - 将此空白留给生成！\n",
        "        ),\n",
        "    ],\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(\n",
        "    **inputs, streamer=text_streamer, max_new_tokens=128, repetition_penalty=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twNf4NXhmLqj"
      },
      "source": [
        "通过使用https://translate.google.com/我们得到\n",
        "```\n",
        "Earth refers to all things including natural disasters such as local derailment\n",
        "\n",
        "and local depletion that occur in one space along with the suppression of water, gases, and living things.\n",
        "\n",
        "Most of the Earth's water comes from oceans, atmospheric water, underground water layers, and rivers and rivers.\n",
        "```\n",
        "\n",
        "yikes语言模型有点烦恼！改变温度并使用采样肯定会使输出变得更好！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "您也可以使用拥抱脸的“ AutomodelForpeftCausAllm”。仅在未安装``'''''时就使用。由于不支持“ 4bit”模型下载，因此它可以慢慢放慢，并且不舒服的**推理快2倍**。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # 我高度建议 - 如果可能的话，请使用不塞\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\",  # 您用于培训的模型\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### 节省float16 vllm\n",
        "\n",
        "我们还直接支持保存到`float16`。为float16选择`merged_16bit`或用于int4的merged_4bit`。我们还允许`Lora`适配器作为后备。使用`push_to_hub_merged`上传到您的拥抱脸部帐户！您可以访问https://huggingface.co/settings/tokens for个人令牌。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# 合并为16位\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# 合并到4位\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# 只是洛拉适配器\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / LLAMA.CPP转换\n",
        "要保存到`gguf' /`llama.cpp`，我们现在在本地支持它！我们克隆“ llama.cpp”，默认将其保存到Q8_0`。我们允许所有方法'q4_k_m`。使用`save_pretained_gguf`进行本地保存，然后`push_to_hub_gguf`将上传到HF。\n",
        "\n",
        "一些支持的量化方法（我们的[Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)上的完整列表）：\n",
        "*`Q8_0`-快速转换。大量资源使用，但通常可以接受。\n",
        "*`q4_k_m`-推荐。使用q6_k作为注意到的一半。\n",
        "*`q5_k_m`-推荐。使用q6_k进行一半的注意。wv和feed_forward.w2张量，else q5_k。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# 保存到8位Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# 保存至16位GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# 保存到Q4_K_M GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q5_k_m\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在，在llama.cpp中使用`model-unsloth.gguf`文件或`model-unsloth-q4_k_m.gguf`文件或基于UI的系统（例如JAN或OPEN webUI）。您可以安装JAN XX_MARKDOWN_LINK_XX并打开webui [here](https://github.com/janhq/jan)\n",
        "\n",
        "我们完成了！如果您对不舒服有任何疑问，我们有一个XX_MarkDown_link_xx频道！如果您发现任何错误或想对最新的LLM内容进行更新，或者需要帮助，加入项目等，请随时加入我们的不和谐！\n",
        "\n",
        "其他一些链接：\n",
        "1。训练您自己的推理模型-Llama grpo笔记本XX_Markdown_link_xx-grpo.ipynb）\n",
        "2。将芬太尼保存到奥尔马。 [here](https://github.com/open-webui/open-webui)-ollama.ipynb）\n",
        "3。Llama3.2视觉燃烧 -  X射线照相用例。 [Discord](https://discord.gg/unsloth)-vision.ipynb）\n",
        "6。请参见DPO，ORPO，继续预处理，对话finetuning等笔记本，以及我们的xx_markDown_link_xx上的更多内容！\n",
        "\n",
        "<div class =“ Align-Center”>\n",
        "  <a href =“ https://unsloth.ai”> <img src =“ https://github.com/unslothai/unslothai/unsloth/raw/raw/main/main/images/unsloth%20new%20new%20logo.png”\n",
        "  <a href =“ https://discord.gg/unsloth”> <img src =“ https://github.com/unslothai/unsloth/unsloth/raw/main/main/main/images/images/discord.png，png\n",
        "  <a href =“ https://docs.unsloth.ai/”> <img src =“ https://github.com/unslothai/unslothai/unsloth/unsloth/main/main/main/images/images/images/documentation%20green%20green%20breen%20button.png?png?png?raw=true width =”\n",
        "\n",
        "  如果您需要帮助，请加入DISCORD +⭐️<i>在<a href =“ https://github.com/unslothai/unsloth上”\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}